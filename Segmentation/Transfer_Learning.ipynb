{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e2363bc",
   "metadata": {},
   "source": [
    "# 한정된 데이터로 Transfer learning 적용해보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e603e6",
   "metadata": {},
   "source": [
    "# 1. CT이미지 데이터셋 살펴보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c290d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73936eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22606e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"./DATASET/Segmentation/\"\n",
    "data_df = pd.read_csv(os.path.join(data_dir, \"train.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e394452",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>MaskId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID00007637202177411956430_0.jpg</td>\n",
       "      <td>ID00007637202177411956430_mask_0.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID00007637202177411956430_1.jpg</td>\n",
       "      <td>ID00007637202177411956430_mask_1.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID00007637202177411956430_2.jpg</td>\n",
       "      <td>ID00007637202177411956430_mask_2.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID00007637202177411956430_3.jpg</td>\n",
       "      <td>ID00007637202177411956430_mask_3.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID00007637202177411956430_4.jpg</td>\n",
       "      <td>ID00007637202177411956430_mask_4.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16703</th>\n",
       "      <td>ID00426637202313170790466_403.jpg</td>\n",
       "      <td>ID00426637202313170790466_mask_403.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16704</th>\n",
       "      <td>ID00426637202313170790466_404.jpg</td>\n",
       "      <td>ID00426637202313170790466_mask_404.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16705</th>\n",
       "      <td>ID00426637202313170790466_405.jpg</td>\n",
       "      <td>ID00426637202313170790466_mask_405.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16706</th>\n",
       "      <td>ID00426637202313170790466_406.jpg</td>\n",
       "      <td>ID00426637202313170790466_mask_406.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16707</th>\n",
       "      <td>ID00426637202313170790466_407.jpg</td>\n",
       "      <td>ID00426637202313170790466_mask_407.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16708 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 ImageId  \\\n",
       "0        ID00007637202177411956430_0.jpg   \n",
       "1        ID00007637202177411956430_1.jpg   \n",
       "2        ID00007637202177411956430_2.jpg   \n",
       "3        ID00007637202177411956430_3.jpg   \n",
       "4        ID00007637202177411956430_4.jpg   \n",
       "...                                  ...   \n",
       "16703  ID00426637202313170790466_403.jpg   \n",
       "16704  ID00426637202313170790466_404.jpg   \n",
       "16705  ID00426637202313170790466_405.jpg   \n",
       "16706  ID00426637202313170790466_406.jpg   \n",
       "16707  ID00426637202313170790466_407.jpg   \n",
       "\n",
       "                                       MaskId  \n",
       "0        ID00007637202177411956430_mask_0.jpg  \n",
       "1        ID00007637202177411956430_mask_1.jpg  \n",
       "2        ID00007637202177411956430_mask_2.jpg  \n",
       "3        ID00007637202177411956430_mask_3.jpg  \n",
       "4        ID00007637202177411956430_mask_4.jpg  \n",
       "...                                       ...  \n",
       "16703  ID00426637202313170790466_mask_403.jpg  \n",
       "16704  ID00426637202313170790466_mask_404.jpg  \n",
       "16705  ID00426637202313170790466_mask_405.jpg  \n",
       "16706  ID00426637202313170790466_mask_406.jpg  \n",
       "16707  ID00426637202313170790466_mask_407.jpg  \n",
       "\n",
       "[16708 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c144057",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_client_id(x):\n",
    "    return x.split('-')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bcb25f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>MaskId</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID00007637202177411956430_0.jpg</td>\n",
       "      <td>ID00007637202177411956430_mask_0.jpg</td>\n",
       "      <td>ID00007637202177411956430_0.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID00007637202177411956430_1.jpg</td>\n",
       "      <td>ID00007637202177411956430_mask_1.jpg</td>\n",
       "      <td>ID00007637202177411956430_1.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID00007637202177411956430_2.jpg</td>\n",
       "      <td>ID00007637202177411956430_mask_2.jpg</td>\n",
       "      <td>ID00007637202177411956430_2.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID00007637202177411956430_3.jpg</td>\n",
       "      <td>ID00007637202177411956430_mask_3.jpg</td>\n",
       "      <td>ID00007637202177411956430_3.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID00007637202177411956430_4.jpg</td>\n",
       "      <td>ID00007637202177411956430_mask_4.jpg</td>\n",
       "      <td>ID00007637202177411956430_4.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16703</th>\n",
       "      <td>ID00426637202313170790466_403.jpg</td>\n",
       "      <td>ID00426637202313170790466_mask_403.jpg</td>\n",
       "      <td>ID00426637202313170790466_403.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16704</th>\n",
       "      <td>ID00426637202313170790466_404.jpg</td>\n",
       "      <td>ID00426637202313170790466_mask_404.jpg</td>\n",
       "      <td>ID00426637202313170790466_404.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16705</th>\n",
       "      <td>ID00426637202313170790466_405.jpg</td>\n",
       "      <td>ID00426637202313170790466_mask_405.jpg</td>\n",
       "      <td>ID00426637202313170790466_405.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16706</th>\n",
       "      <td>ID00426637202313170790466_406.jpg</td>\n",
       "      <td>ID00426637202313170790466_mask_406.jpg</td>\n",
       "      <td>ID00426637202313170790466_406.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16707</th>\n",
       "      <td>ID00426637202313170790466_407.jpg</td>\n",
       "      <td>ID00426637202313170790466_mask_407.jpg</td>\n",
       "      <td>ID00426637202313170790466_407.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16708 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 ImageId  \\\n",
       "0        ID00007637202177411956430_0.jpg   \n",
       "1        ID00007637202177411956430_1.jpg   \n",
       "2        ID00007637202177411956430_2.jpg   \n",
       "3        ID00007637202177411956430_3.jpg   \n",
       "4        ID00007637202177411956430_4.jpg   \n",
       "...                                  ...   \n",
       "16703  ID00426637202313170790466_403.jpg   \n",
       "16704  ID00426637202313170790466_404.jpg   \n",
       "16705  ID00426637202313170790466_405.jpg   \n",
       "16706  ID00426637202313170790466_406.jpg   \n",
       "16707  ID00426637202313170790466_407.jpg   \n",
       "\n",
       "                                       MaskId  \\\n",
       "0        ID00007637202177411956430_mask_0.jpg   \n",
       "1        ID00007637202177411956430_mask_1.jpg   \n",
       "2        ID00007637202177411956430_mask_2.jpg   \n",
       "3        ID00007637202177411956430_mask_3.jpg   \n",
       "4        ID00007637202177411956430_mask_4.jpg   \n",
       "...                                       ...   \n",
       "16703  ID00426637202313170790466_mask_403.jpg   \n",
       "16704  ID00426637202313170790466_mask_404.jpg   \n",
       "16705  ID00426637202313170790466_mask_405.jpg   \n",
       "16706  ID00426637202313170790466_mask_406.jpg   \n",
       "16707  ID00426637202313170790466_mask_407.jpg   \n",
       "\n",
       "                                      id  \n",
       "0        ID00007637202177411956430_0.jpg  \n",
       "1        ID00007637202177411956430_1.jpg  \n",
       "2        ID00007637202177411956430_2.jpg  \n",
       "3        ID00007637202177411956430_3.jpg  \n",
       "4        ID00007637202177411956430_4.jpg  \n",
       "...                                  ...  \n",
       "16703  ID00426637202313170790466_403.jpg  \n",
       "16704  ID00426637202313170790466_404.jpg  \n",
       "16705  ID00426637202313170790466_405.jpg  \n",
       "16706  ID00426637202313170790466_406.jpg  \n",
       "16707  ID00426637202313170790466_407.jpg  \n",
       "\n",
       "[16708 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df['id'] = data_df.ImageId.apply(lambda x: extract_client_id(x))\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3802c442",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_client_data(data_df, index):\n",
    "    client_ids = np.unique(data_df.id.values)\n",
    "    client_id = client_ids[index]\n",
    "    client_data = data_df[data_df.id == client_id]\n",
    "    Image_files = list(client_data['ImageId'])\n",
    "    mask_files = list(client_data['MaskId'])\n",
    "    return client_id, Image_files, mask_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5225e028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ID00009637202177434476278_161.jpg',\n",
       " ['ID00009637202177434476278_161.jpg'],\n",
       " ['ID00009637202177434476278_mask_161.jpg'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = 100\n",
    "get_client_data(data_df, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6d35122",
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = [\"background\", \"trachea\", \"heart\", \"lung\"]\n",
    "colors = ((0,0,0), (255,0,0), (0,255,0), (0,0,255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05f7e5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 50\n",
    "client_id, image_files , mask_files = get_client_data(data_df, index)\n",
    "\n",
    "canvas = np.zeros(shape=(512, 2*512+50, 3), dtype=np.uint8)\n",
    "for i in range(len(image_files)):\n",
    "    image = cv2.imread(os.path.join(data_dir, \"images\", image_files[i]))\n",
    "    mask = cv2.imread(os.path.join(data_dir, \"masks\", mask_files[i]))\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    mask = cv2.cvtColor(mask, cv2.COLOR_BGR2RGB)\n",
    "    mask[mask<240] = 0\n",
    "    mask[mask>=240] = 255\n",
    "    \n",
    "    canvas[:, 0:512, :] = image\n",
    "    canvas[:, 512+50:2*512+50, :] = mask\n",
    "    \n",
    "    cv2.imshow('image', canvas)\n",
    "    key = cv2.waitKey(100)\n",
    "    if key == 27:\n",
    "        break\n",
    "    if key == ord('s'):\n",
    "        cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1621936",
   "metadata": {},
   "source": [
    "# 2. 데이터셋 구축과 연산을 위한 텐서변환 모듈 작성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7d08951",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47fe150f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CT_dataset():\n",
    "    def __init__(self, data_dir, phase, transformer=None):\n",
    "        self.phase = phase\n",
    "        self.images_dir = os.path.join(data_dir, phase, 'images')\n",
    "        self.masks_dir = os.path.join(data_dir, phase, 'masks')\n",
    "        self.image_files = [filename for filename in os.listdir(self.images_dir) if filename.endswith('jpg')]\n",
    "        self.mask_files = [filename for filename in os.listdir(self.masks_dir) if filename.endswith('jpg')]\n",
    "        assert len(self.image_files) == len(self.mask_files)\n",
    "        \n",
    "        self.transformer = transformer\n",
    "        \n",
    "    def __len__(self,):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image = cv2.imread(os.path.join(self.images_dir, self.image_files[index]))\n",
    "        image = cv2.resize(image, dsize=(IMAGE_SIZE, IMAGE_SIZE), interpolation=cv2.INTER_LINEAR)\n",
    "        mask = cv2.imread(os.path.join(self.masks_dir, self.mask_files[index]))\n",
    "        mask = cv2.resize(mask, dsize=(IMAGE_SIZE, IMAGE_SIZE), interpolation=cv2.INTER_NEAREST)\n",
    "        \n",
    "        mask[mask < 240] = 0\n",
    "        mask[mask >= 240] = 255\n",
    "        mask = mask / 255\n",
    "        \n",
    "        mask_H, mask_W, mask_C = mask.shape\n",
    "        background = np.ones(shape=(mask_H, mask_W))\n",
    "        background[mask[..., 0] != 0] = 0\n",
    "        background[mask[..., 1] != 0] = 0\n",
    "        background[mask[..., 2] != 0] = 0\n",
    "        \n",
    "        mask = np.concatenate([np.expand_dims(background, axis=-1), mask], axis=-1)\n",
    "        \"\"\"\n",
    "        background shape: (H, W) -> shape 확장 (H, W, 1)로\n",
    "        mask shape: (H, W, C)\n",
    "        \n",
    "        new mask shape: (H, W, C+1)\n",
    "        \"\"\"\n",
    "        mask = np.argmax(mask, axis=-1)\n",
    "        \"new mask shape: (H, W, 4) -> new mask shape: (H, W)\"\n",
    "        \n",
    "        if self.transformer:\n",
    "            image = self.transformer(image)\n",
    "            \n",
    "        target = torch.from_numpy(mask).long()\n",
    "        \n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "29e1dcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = CT_dataset(data_dir, \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "237b41c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((224, 224, 3), torch.Size([224, 224]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image, target = dset[0]\n",
    "image.shape, target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4a0e0531",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "def build_transformer():\n",
    "    transformer = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    return transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "213baecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    images = []\n",
    "    targets = []\n",
    "    for a, b in batch:\n",
    "        images.append(a)\n",
    "        targets.append(b)\n",
    "    images = torch.stack(images, dim=0)\n",
    "    targets = torch.stack(targets, dim=0)\n",
    "    return images, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "293c88c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = build_transformer()\n",
    "dset = CT_dataset(data_dir=data_dir, phase=\"train\", transformer=transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2f441a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape: torch.Size([3, 224, 224])\n",
      "target shape: torch.Size([224, 224])\n"
     ]
    }
   ],
   "source": [
    "image, target = dset[0]\n",
    "print(f\"image shape: {image.shape}\")\n",
    "print(f\"target shape: {target.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4c6688a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c93d7e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dloader = DataLoader(dset, batch_size=4, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "970241a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape: torch.Size([4, 3, 224, 224])\n",
      "targets shape: torch.Size([4, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "for index, batch in enumerate(dloader):\n",
    "    images = batch[0]\n",
    "    targets = batch[1]\n",
    "    print(f\"images shape: {images.shape}\")\n",
    "    print(f\"targets shape: {targets.shape}\")\n",
    "    \n",
    "    if index == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "23c3c108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataloader(data_dir, batch_size=4):\n",
    "    transformer = build_transformer()\n",
    "    \n",
    "    dataloaders = {}\n",
    "    train_dataset = CT_dataset(data_dir=data_dir, phase=\"train\", transformer=transformer)\n",
    "    dataloaders[\"train\"] = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "    \n",
    "    val_dataset = CT_dataset(data_dir=data_dir, phase=\"val\", transformer=transformer)\n",
    "    dataloaders[\"val\"] = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "    return dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "797c0cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape: torch.Size([4, 3, 224, 224])\n",
      "targets shape: torch.Size([4, 224, 224])\n",
      "images shape: torch.Size([4, 3, 224, 224])\n",
      "targets shape: torch.Size([4, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "dataloaders = build_dataloader(data_dir=data_dir)\n",
    "\n",
    "for phase in [\"train\", \"val\"]:\n",
    "    for index, batch in enumerate(dataloaders[phase]):\n",
    "        images = batch[0]\n",
    "        targets = batch[1]\n",
    "        print(f\"images shape: {images.shape}\")\n",
    "        print(f\"targets shape: {targets.shape}\")\n",
    "        \n",
    "        if index == 0:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9faf8ee8",
   "metadata": {},
   "source": [
    "# 3. VGG16 Backbone을 이용한 U-Net 아키텍쳐 구현해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7b939fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "54e0ef60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvLayer(in_channels, out_channels, kernel_size=3, padding=1):\n",
    "    layers = nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=padding),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True),\n",
    "    \n",
    "        nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, padding=padding),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True),\n",
    "    )\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d19a99a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def UpConvLayer(in_channels, out_channels):\n",
    "    layers = nn.Sequential(\n",
    "        nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d5ce0c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0c88f124",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (12): ReLU(inplace=True)\n",
       "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (16): ReLU(inplace=True)\n",
       "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (19): ReLU(inplace=True)\n",
       "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (26): ReLU(inplace=True)\n",
       "    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (32): ReLU(inplace=True)\n",
       "    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (36): ReLU(inplace=True)\n",
       "    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (39): ReLU(inplace=True)\n",
       "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (42): ReLU(inplace=True)\n",
       "    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg16 = models.vgg16_bn(pretrained=False)\n",
    "vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5f0360eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, pretrained):\n",
    "        super().__init__()\n",
    "        \n",
    "        backbone = models.vgg16_bn(pretrained=pretrained).features\n",
    "        self.conv_block1 = nn.Sequential(*backbone[:6])\n",
    "        self.conv_block2 = nn.Sequential(*backbone[6:13])\n",
    "        self.conv_block3 = nn.Sequential(*backbone[13:20])\n",
    "        self.conv_block4 = nn.Sequential(*backbone[20:27])\n",
    "        self.conv_block5 = nn.Sequential(*backbone[27:34],\n",
    "                                        ConvLayer(512, 1024, kernel_size=1, padding=0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encode_features = []\n",
    "        out = self.conv_block1(x)\n",
    "        encode_features.append(out)\n",
    "        \n",
    "        out = self.conv_block2(out)\n",
    "        encode_features.append(out)\n",
    "        \n",
    "        out = self.conv_block3(out)\n",
    "        encode_features.append(out)\n",
    "        \n",
    "        out = self.conv_block4(out)\n",
    "        encode_features.append(out)\n",
    "        \n",
    "        out = self.conv_block5(out)\n",
    "        return out, encode_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5464204e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "[W NNPACK.cpp:51] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(pretrained=False)\n",
    "x = torch.randn(1, 3, 224, 224)\n",
    "out, ftrs = encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "704bad15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 224, 224])\n",
      "torch.Size([1, 128, 112, 112])\n",
      "torch.Size([1, 256, 56, 56])\n",
      "torch.Size([1, 512, 28, 28])\n",
      "torch.Size([1, 1024, 14, 14])\n"
     ]
    }
   ],
   "source": [
    "for ftr in ftrs:\n",
    "    print(ftr.shape)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "193efac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.upconv_layer1 = UpConvLayer(in_channels=1024, out_channels=512)\n",
    "        self.conv_block1 = ConvLayer(in_channels=512+512, out_channels=512)\n",
    "        \n",
    "        self.upconv_layer2 = UpConvLayer(in_channels=512, out_channels=256)\n",
    "        self.conv_block2 = ConvLayer(in_channels=256+256, out_channels=256)\n",
    "        \n",
    "        self.upconv_layer3 = UpConvLayer(in_channels=256, out_channels=128)\n",
    "        self.conv_block3 = ConvLayer(in_channels=128+128, out_channels=128)\n",
    "        \n",
    "        self.upconv_layer4 = UpConvLayer(in_channels=128, out_channels=64)\n",
    "        self.conv_block4 = ConvLayer(in_channels=64+64, out_channels=64)\n",
    "        \n",
    "    def forward(self, x, encoder_features):\n",
    "        out = self.upconv_layer1(x)\n",
    "        out = torch.cat([out, encoder_features[-1]], dim=1)\n",
    "        out = self.conv_block1(out)\n",
    "        \n",
    "        out = self.upconv_layer2(out)\n",
    "        out = torch.cat([out, encoder_features[-2]], dim=1)\n",
    "        out = self.conv_block2(out)\n",
    "        \n",
    "        out = self.upconv_layer3(out)\n",
    "        out = torch.cat([out, encoder_features[-3]], dim=1)\n",
    "        out = self.conv_block3(out)\n",
    "        \n",
    "        out = self.upconv_layer4(out)\n",
    "        out = torch.cat([out, encoder_features[-4]], dim=1)\n",
    "        out = self.conv_block4(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f8c5887a",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(pretrained=False)\n",
    "decoder = Decoder()\n",
    "x = torch.randn(1, 3, 224, 224)\n",
    "out, ftrs = encoder(x)\n",
    "out = decoder(out, ftrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1e6570f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e89cfa46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, num_classes, pretrained):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(pretrained)\n",
    "        self.decoder = Decoder()\n",
    "        self.head = nn.Conv2d(64, num_classes, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out, encode_features = self.encoder(x)\n",
    "        out = self.decoder(out, encode_features)\n",
    "        out = self.head(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "53d681c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(num_classes=4, pretrained=False)\n",
    "x = torch.randn(1, 3, 224, 224)\n",
    "out = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e57bef16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9730808",
   "metadata": {},
   "source": [
    "# 4. Semantic segmentation Loss와 학습코드 작성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0cd78e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c648a821",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet_metric():\n",
    "    def __init__(self, num_classes):\n",
    "        self.num_classes = num_classes\n",
    "        self.CE_loss = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "        \n",
    "    def __call__(self, pred, target):\n",
    "        loss1 = self.CE_loss(pred, target)\n",
    "        onehot_pred = F.one_hot(torch.argmax(pred, dim=1), num_classes=self.num_classes).permute(0, 3, 1, 2)\n",
    "        onehot_target = F.one_hot(target, num_classes=self.num_classes).permute(0, 3, 1, 2)\n",
    "        loss2 = self._get_dice_loss(onehot_pred, onehot_target)\n",
    "        loss = loss1 + loss2\n",
    "        \n",
    "        dice_coefficient = self._get_batch_dice_coefficient(onehot_pred, onehot_target)\n",
    "        return loss, dice_coefficient\n",
    "    \n",
    "    def _get_dice_coeffient(self, pred, target):\n",
    "        set_inter = torch.dot(pred.reshape(-1).float(), target.reshape(-1).float())\n",
    "        set_sum = pred.sum() + target.sum()\n",
    "        if set_sum.item() == 0:\n",
    "            set_sum = 2 * set_inter\n",
    "        dice_coeff = (2 * set_inter) / (set_sum + 1e-9)\n",
    "        return dice_coeff\n",
    "    \n",
    "    def _get_multiclass_dice_coefficient(self, pred, target):\n",
    "        dice = 0\n",
    "        for class_index in range(1, self.num_classes):\n",
    "            dice += self._get_dice_coeffient(pred[class_index], target[class_index])\n",
    "        return dice / (self.num_classes - 1)\n",
    "    \n",
    "    def _get_batch_dice_coefficient(self, pred, target):\n",
    "        num_batch = pred.shape[0]\n",
    "        dice = 0\n",
    "        for batch_index in range(num_batch):\n",
    "            dice += self._get_multiclass_dice_coefficient(pred[batch_index], target[batch_index])\n",
    "        return dice / num_batch\n",
    "    \n",
    "    def _get_dice_loss(self, pred, target):\n",
    "        return 1 - self._get_batch_dice_coefficient(pred, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b3873ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(dataloaders, model, criterion, optimizer, device):\n",
    "    losses = {}\n",
    "    dice_coefficients = {}\n",
    "    \n",
    "    for phase in [\"train\", \"val\"]:\n",
    "        running_loss = 0.0\n",
    "        running_dice_coeff = 0.0\n",
    "        \n",
    "        if phase == \"train\":\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "        \n",
    "        for index, batch in enumerate(dataloaders[phase]):\n",
    "            images = batch[0].to(device)\n",
    "            targets = batch[1].to(device)\n",
    "            \n",
    "            with torch.set_grad_enabled(phase == \"train\"):\n",
    "                predictions = model(images)\n",
    "                loss, dice_coefficient = criterion(predictions, targets)\n",
    "                \n",
    "            if phase == \"train\":\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            running_dice_coeff += dice_coefficient.item()\n",
    "\n",
    "            if index == 10: # 10 index * mini_batch 데이터수 만큼 데이터를 한정\n",
    "                break\n",
    "\n",
    "        losses[phase] = running_loss / index\n",
    "        dice_coefficients[phase] = running_dice_coeff / index\n",
    "        \n",
    "    return losses, dice_coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c161a3",
   "metadata": {},
   "source": [
    "# 5. Weight Initialization 과 Transfer learning 모델 비교하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4caa7c9c",
   "metadata": {},
   "source": [
    "## 5-1 He initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "330c8831",
   "metadata": {},
   "outputs": [],
   "source": [
    "def He_initialization(module):\n",
    "    if isinstance(module, nn.Conv2d):\n",
    "        nn.init.kaiming_normal_(module.weight)\n",
    "    elif isinstance(module, nn.BatchNorm2d):\n",
    "        module.weight.data.fill_(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "648335d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "is_mps = True\n",
    "\n",
    "NUM_CLASSES = 4\n",
    "IMAGE_SIZE = 224\n",
    "BATCH_SIZE = 12\n",
    "DEVICE = torch.device('mps' if torch.backends.mps.is_available() and is_mps else 'cpu')\n",
    "\n",
    "dataloaders = build_dataloader(data_dir=data_dir, batch_size=BATCH_SIZE)\n",
    "model = UNet(num_classes=NUM_CLASSES, pretrained=False)\n",
    "model.apply(He_initialization)\n",
    "model = model.to(DEVICE)\n",
    "criterion = UNet_metric(num_classes=NUM_CLASSES)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bebb28ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/30 - Train loss: 2.3117, Val loss: 2.0145, Train dice: 0.0148, Val dice: 0.0056\n",
      "1/30 - Train loss: 2.0234, Val loss: 1.8231, Train dice: 0.0114, Val dice: 0.0046\n",
      "2/30 - Train loss: 1.8082, Val loss: 1.7219, Train dice: 0.0058, Val dice: 0.0030\n",
      "3/30 - Train loss: 1.6929, Val loss: 1.6525, Train dice: 0.0025, Val dice: 0.0018\n",
      "4/30 - Train loss: 1.6342, Val loss: 1.6133, Train dice: 0.0012, Val dice: 0.0011\n",
      "5/30 - Train loss: 1.6022, Val loss: 1.5905, Train dice: 0.0007, Val dice: 0.0007\n",
      "6/30 - Train loss: 1.5824, Val loss: 1.5759, Train dice: 0.0004, Val dice: 0.0006\n",
      "7/30 - Train loss: 1.5687, Val loss: 1.5657, Train dice: 0.0003, Val dice: 0.0005\n",
      "8/30 - Train loss: 1.5584, Val loss: 1.5579, Train dice: 0.0002, Val dice: 0.0004\n",
      "9/30 - Train loss: 1.5502, Val loss: 1.5515, Train dice: 0.0002, Val dice: 0.0004\n",
      "10/30 - Train loss: 1.5435, Val loss: 1.5463, Train dice: 0.0002, Val dice: 0.0004\n",
      "11/30 - Train loss: 1.5378, Val loss: 1.5418, Train dice: 0.0002, Val dice: 0.0004\n",
      "12/30 - Train loss: 1.5329, Val loss: 1.5379, Train dice: 0.0002, Val dice: 0.0004\n",
      "13/30 - Train loss: 1.5287, Val loss: 1.5344, Train dice: 0.0001, Val dice: 0.0003\n",
      "14/30 - Train loss: 1.5248, Val loss: 1.5312, Train dice: 0.0001, Val dice: 0.0003\n",
      "15/30 - Train loss: 1.5214, Val loss: 1.5283, Train dice: 0.0001, Val dice: 0.0003\n",
      "16/30 - Train loss: 1.5183, Val loss: 1.5256, Train dice: 0.0001, Val dice: 0.0003\n",
      "17/30 - Train loss: 1.5154, Val loss: 1.5232, Train dice: 0.0001, Val dice: 0.0003\n",
      "18/30 - Train loss: 1.5128, Val loss: 1.5208, Train dice: 0.0001, Val dice: 0.0003\n",
      "19/30 - Train loss: 1.5104, Val loss: 1.5185, Train dice: 0.0001, Val dice: 0.0003\n",
      "20/30 - Train loss: 1.5081, Val loss: 1.5164, Train dice: 0.0001, Val dice: 0.0003\n",
      "21/30 - Train loss: 1.5060, Val loss: 1.5144, Train dice: 0.0001, Val dice: 0.0003\n",
      "22/30 - Train loss: 1.5040, Val loss: 1.5125, Train dice: 0.0001, Val dice: 0.0003\n",
      "23/30 - Train loss: 1.5021, Val loss: 1.5107, Train dice: 0.0001, Val dice: 0.0003\n",
      "24/30 - Train loss: 1.5003, Val loss: 1.5089, Train dice: 0.0001, Val dice: 0.0003\n",
      "25/30 - Train loss: 1.4986, Val loss: 1.5072, Train dice: 0.0001, Val dice: 0.0003\n",
      "26/30 - Train loss: 1.4969, Val loss: 1.5056, Train dice: 0.0001, Val dice: 0.0003\n",
      "27/30 - Train loss: 1.4953, Val loss: 1.5040, Train dice: 0.0001, Val dice: 0.0004\n",
      "28/30 - Train loss: 1.4938, Val loss: 1.5024, Train dice: 0.0001, Val dice: 0.0004\n",
      "29/30 - Train loss: 1.4924, Val loss: 1.5009, Train dice: 0.0001, Val dice: 0.0004\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "\n",
    "train_loss_def, train_dice_coefficient_def = [], []\n",
    "val_loss_def, val_dice_coefficient_def = [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    losses, dice_coefficients = train_one_epoch(dataloaders, model, criterion, optimizer, DEVICE)\n",
    "    train_loss_def.append(losses['train'])\n",
    "    val_loss_def.append(losses['val'])\n",
    "    train_dice_coefficient_def.append(dice_coefficients['train'])\n",
    "    val_dice_coefficient_def.append(dice_coefficients['val'])\n",
    "    \n",
    "    print(f\"{epoch}/{num_epochs} - Train loss: {losses['train']:.4f}, Val loss: {losses['val']:.4f},\" + \\\n",
    "          f\" Train dice: {dice_coefficients['train']:.4f}, Val dice: {dice_coefficients['val']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fa4249",
   "metadata": {},
   "source": [
    "## 5-2 Weight transfer pre-trained on ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "87c17891",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG16_BN_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_BN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/vgg16_bn-6c64b313.pth\" to /Users/jeong-wonseok/.cache/torch/hub/checkpoints/vgg16_bn-6c64b313.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cebef629965045d3bda0fbc8158c201b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/528M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataloaders = build_dataloader(data_dir=data_dir, batch_size=BATCH_SIZE)\n",
    "model = UNet(num_classes=NUM_CLASSES, pretrained=True)\n",
    "model = model.to(DEVICE)\n",
    "criterion = UNet_metric(num_classes=NUM_CLASSES)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a63d7495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/30 - Train loss: 2.4013, Val loss: 2.4771, Train dice: 0.0426, Val dice: 0.0001\n",
      "1/30 - Train loss: 2.0697, Val loss: 2.2692, Train dice: 0.0218, Val dice: 0.0000\n",
      "2/30 - Train loss: 1.8071, Val loss: 1.9968, Train dice: 0.0015, Val dice: 0.0000\n",
      "3/30 - Train loss: 1.6656, Val loss: 1.7577, Train dice: 0.0000, Val dice: 0.0000\n",
      "4/30 - Train loss: 1.6008, Val loss: 1.6240, Train dice: 0.0000, Val dice: 0.0000\n",
      "5/30 - Train loss: 1.5688, Val loss: 1.5640, Train dice: 0.0000, Val dice: 0.0000\n",
      "6/30 - Train loss: 1.5499, Val loss: 1.5379, Train dice: 0.0000, Val dice: 0.0000\n",
      "7/30 - Train loss: 1.5369, Val loss: 1.5252, Train dice: 0.0000, Val dice: 0.0000\n",
      "8/30 - Train loss: 1.5272, Val loss: 1.5177, Train dice: 0.0000, Val dice: 0.0000\n",
      "9/30 - Train loss: 1.5178, Val loss: 1.5127, Train dice: 0.0000, Val dice: 0.0000\n",
      "10/30 - Train loss: 1.5092, Val loss: 1.5083, Train dice: 0.0000, Val dice: 0.0000\n",
      "11/30 - Train loss: 1.5011, Val loss: 1.5039, Train dice: 0.0000, Val dice: 0.0000\n",
      "12/30 - Train loss: 1.4931, Val loss: 1.4995, Train dice: 0.0000, Val dice: 0.0000\n",
      "13/30 - Train loss: 1.4857, Val loss: 1.4949, Train dice: 0.0000, Val dice: 0.0000\n",
      "14/30 - Train loss: 1.4766, Val loss: 1.4898, Train dice: 0.0000, Val dice: 0.0000\n",
      "15/30 - Train loss: 1.4705, Val loss: 1.4825, Train dice: 0.0000, Val dice: 0.0000\n",
      "16/30 - Train loss: 1.4601, Val loss: 1.4781, Train dice: 0.0000, Val dice: 0.0000\n",
      "17/30 - Train loss: 1.4499, Val loss: 1.4700, Train dice: 0.0000, Val dice: 0.0000\n",
      "18/30 - Train loss: 1.4389, Val loss: 1.4659, Train dice: 0.0000, Val dice: 0.0000\n",
      "19/30 - Train loss: 1.4284, Val loss: 1.4600, Train dice: 0.0000, Val dice: 0.0000\n",
      "20/30 - Train loss: 1.4176, Val loss: 1.4567, Train dice: 0.0001, Val dice: 0.0000\n",
      "21/30 - Train loss: 1.4078, Val loss: 1.4569, Train dice: 0.0003, Val dice: 0.0000\n",
      "22/30 - Train loss: 1.3988, Val loss: 1.4545, Train dice: 0.0010, Val dice: 0.0000\n",
      "23/30 - Train loss: 1.3866, Val loss: 1.4531, Train dice: 0.0033, Val dice: 0.0000\n",
      "24/30 - Train loss: 1.3725, Val loss: 1.4539, Train dice: 0.0088, Val dice: 0.0002\n",
      "25/30 - Train loss: 1.3511, Val loss: 1.4527, Train dice: 0.0204, Val dice: 0.0007\n",
      "26/30 - Train loss: 1.3131, Val loss: 1.4537, Train dice: 0.0424, Val dice: 0.0018\n",
      "27/30 - Train loss: 1.2689, Val loss: 1.4553, Train dice: 0.0721, Val dice: 0.0037\n",
      "28/30 - Train loss: 1.2255, Val loss: 1.4565, Train dice: 0.1026, Val dice: 0.0063\n",
      "29/30 - Train loss: 1.1881, Val loss: 1.4587, Train dice: 0.1287, Val dice: 0.0092\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "\n",
    "train_loss_prt, train_dice_coefficient_prt = [], []\n",
    "val_loss_prt, val_dice_coefficient_prt = [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    losses, dice_coefficients = train_one_epoch(dataloaders, model, criterion, optimizer, DEVICE)\n",
    "    train_loss_prt.append(losses[\"train\"])\n",
    "    val_loss_prt.append(losses[\"val\"])\n",
    "    train_dice_coefficient_prt.append(dice_coefficients[\"train\"])\n",
    "    val_dice_coefficient_prt.append(dice_coefficients[\"val\"])\n",
    "    \n",
    "    print(f\"{epoch}/{num_epochs} - Train loss: {losses['train']:.4f}, Val loss: {losses['val']:.4f},\" + \\\n",
    "          f\" Train dice: {dice_coefficients['train']:.4f}, Val dice: {dice_coefficients['val']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699aa7e5",
   "metadata": {},
   "source": [
    "## 5-3 Weight transfer with freezing encoder layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "af408464",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = build_dataloader(data_dir=data_dir, batch_size=BATCH_SIZE)\n",
    "model = UNet(num_classes=NUM_CLASSES, pretrained=True)\n",
    "model = model.to(DEVICE)\n",
    "model.encoder.requires_grad_ = False\n",
    "criterion = UNet_metric(num_classes=NUM_CLASSES)\n",
    "optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8b10db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/30 - Train loss: 2.2997, Val loss: 2.4209, Train dice: 0.0345, Val dice: 0.0000\n",
      "1/30 - Train loss: 2.0216, Val loss: 2.1491, Train dice: 0.0098, Val dice: 0.0000\n",
      "2/30 - Train loss: 1.7879, Val loss: 1.8848, Train dice: 0.0002, Val dice: 0.0000\n",
      "3/30 - Train loss: 1.6630, Val loss: 1.6991, Train dice: 0.0000, Val dice: 0.0000\n",
      "4/30 - Train loss: 1.6039, Val loss: 1.6033, Train dice: 0.0000, Val dice: 0.0000\n",
      "5/30 - Train loss: 1.5733, Val loss: 1.5600, Train dice: 0.0000, Val dice: 0.0000\n",
      "6/30 - Train loss: 1.5546, Val loss: 1.5394, Train dice: 0.0000, Val dice: 0.0000\n",
      "7/30 - Train loss: 1.5415, Val loss: 1.5285, Train dice: 0.0000, Val dice: 0.0000\n",
      "8/30 - Train loss: 1.5318, Val loss: 1.5203, Train dice: 0.0000, Val dice: 0.0000\n",
      "9/30 - Train loss: 1.5239, Val loss: 1.5154, Train dice: 0.0000, Val dice: 0.0000\n",
      "10/30 - Train loss: 1.5149, Val loss: 1.5103, Train dice: 0.0000, Val dice: 0.0000\n",
      "11/30 - Train loss: 1.5069, Val loss: 1.5051, Train dice: 0.0000, Val dice: 0.0000\n",
      "12/30 - Train loss: 1.4997, Val loss: 1.5013, Train dice: 0.0000, Val dice: 0.0000\n",
      "13/30 - Train loss: 1.4940, Val loss: 1.4986, Train dice: 0.0000, Val dice: 0.0000\n",
      "14/30 - Train loss: 1.4856, Val loss: 1.4918, Train dice: 0.0000, Val dice: 0.0000\n",
      "15/30 - Train loss: 1.4754, Val loss: 1.4866, Train dice: 0.0000, Val dice: 0.0000\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "\n",
    "train_loss_frz, train_dice_coefficient_frz = [], []\n",
    "val_loss_frz, val_dice_coefficient_frz = [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    losses, dice_coefficients = train_one_epoch(dataloaders, model, criterion, optimizer, DEVICE)\n",
    "    train_loss_frz.append(losses[\"train\"])\n",
    "    val_loss_frz.append(losses[\"val\"])\n",
    "    train_dice_coefficient_frz.append(dice_coefficients[\"train\"])\n",
    "    val_dice_coefficient_frz.append(dice_coefficients[\"val\"])\n",
    "    \n",
    "    print(f\"{epoch}/{num_epochs} - Train loss: {losses['train']:.4f}, Val loss: {losses['val']:.4f},\" + \\\n",
    "          f\" Train dice: {dice_coefficients['train']:.4f}, Val dice: {dice_coefficients['val']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d014cceb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
